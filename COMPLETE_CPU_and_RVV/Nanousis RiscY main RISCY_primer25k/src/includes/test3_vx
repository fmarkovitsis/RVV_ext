//vsetivli rd, uimm, vtypei 
//vadd.vx vd, vs2, rs1
//addi t0, t1, +imm


//No forwarding (working)
1_1_0001010000_00010_111_11111_1010111 //vsetvli 11111, 00010, 0001010000 //SEW=32, LMUL=1, AVL=2

000000000001_01011_000_01011_0010011 //addi 01011, 01011, 000000000001  //r11 = r11 + 1 //r11 = 1
000000000100_01110_000_01110_0010011 //addi 01110, 01110, 000000000100  //r14 = r14 + 4 //r14 = 4
000000000110_10000_000_10000_0010011 //addi 10000, 10000, 000000000110  //r16 = r16 + 6 //r16 = 6
000000001000_10010_000_10010_0010011 //addi 10010, 10010, 000000001000  //r18 = r18 + 8 //r18 = 8

000000_0_00000_00001_011_00000_1010111 //vadd.vi 00000, 00000, 00001 //v0 = v0 + 1   //v0 = 00000001_00000001
000000_0_00000_01011_100_00000_1010111 //vadd.vx 00000, 00000, 01011 //v0 = v0 + r11 //v0 = 00000002_00000002
000000_0_00001_01110_100_00001_1010111 //vadd.vx 00001, 00001, 01110 //v1 = v1 + r14 //v1 = 00000004_00000004
000000_0_00000_00001_000_00100_1010111 //vadd.vv 00000, 00001, 00100 //v4 = v0 + v1  //v4 = 00000006_00000006
000000_0_00010_10000_100_00010_1010111 //vadd.vx 00010, 00010, 10000 //v2 = v2 + r16 //v2 = 00000006_00000006
000000_0_00011_10010_100_00011_1010111 //vadd.vx 00011, 00011, 10010 //v3 = v3 + r18 //v3 = 00000008_00000008
000000_0_00010_00011_000_00101_1010111 //vadd.vv 00010, 00011, 00101 //v5 = v2 + v3  //v5 = 0000000E_0000000E
000000_0_00100_00101_000_00110_1010111 //vadd.vv 00100, 00101, 00110 //v6 = v4 + v5  //v6 = 00000014_00000014
000000_0_00110_00110_000_00111_1010111 //vadd.vv 00110, 00110, 00111 //v7 = v6 + v6  //v7 = 00000028_00000028






/*************************/
//Yes forwarding (working)
1_1_0001010000_00010_111_11111_1010111 //vsetvli 11111, 00010, 0001010000 //SEW=32, LMUL=1, AVL=2

000000_0_00000_00001_011_00000_1010111 //vadd.vi 00000, 00000, 00001 //v0 = v0 + 1   //v0 = 00000001_00000001

000000000001_01011_000_01011_0010011 //addi 01011, 01011, 000000000001  //r11 = r11 + 1 //r11 = 1
000000_0_00000_01011_100_00000_1010111 //vadd.vx 00000, 00000, 01011 //v0 = v0 + r11 //v0 = 00000002_00000002

000000000100_01110_000_01110_0010011 //addi 01110, 01110, 000000000100  //r14 = r14 + 4 //r14 = 4
000000_0_00001_01110_100_00001_1010111 //vadd.vx 00001, 00001, 01110 //v1 = v1 + r14 //v1 = 00000004_00000004

000000_0_00000_00001_000_00100_1010111 //vadd.vv 00000, 00001, 00100 //v4 = v0 + v1  //v4 = 00000006_00000006

000000000110_10000_000_10000_0010011 //addi 10000, 10000, 000000000110  //r16 = r16 + 6 //r16 = 6
000000_0_00010_10000_100_00010_1010111 //vadd.vx 00010, 00010, 10000 //v2 = v2 + r16 //v2 = 00000006_00000006

000000001000_10010_000_10010_0010011 //addi 10010, 10010, 000000001000  //r18 = r18 + 8 //r18 = 8
000000_0_00011_10010_100_00011_1010111 //vadd.vx 00011, 00011, 10010 //v3 = v3 + r18 //v3 = 00000008_00000008

000000_0_00010_00011_000_00101_1010111 //vadd.vv 00010, 00011, 00101 //v5 = v2 + v3  //v5 = 0000000E_0000000E
000000_0_00100_00101_000_00110_1010111 //vadd.vv 00100, 00101, 00110 //v6 = v4 + v5  //v6 = 00000014_00000014
000000_0_00110_00110_000_00111_1010111 //vadd.vv 00110, 00110, 00111 //v7 = v6 + v6  //v7 = 00000028_00000028